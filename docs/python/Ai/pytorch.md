# Pytorch 

## Pytorch åŸºç¡€

### 1. ä»€ä¹ˆæ˜¯Pytorch

PyTorchæ˜¯ä¸€ä¸ªå¼€æºçš„Pythonæœºå™¨å­¦ä¹ åº“ï¼ŒåŸºäºTorchåº“ï¼ˆä¸€ä¸ªæœ‰å¤§é‡æœºå™¨å­¦ä¹ ç®—æ³•æ”¯æŒçš„ç§‘å­¦è®¡ç®—æ¡†æ¶ï¼Œæœ‰ç€ä¸Numpyç±»ä¼¼çš„å¼ é‡ï¼ˆTensorï¼‰æ“ä½œï¼Œé‡‡ç”¨çš„ç¼–ç¨‹è¯­è¨€æ˜¯Luaï¼‰ï¼Œåº•å±‚ç”±C++å®ç°ï¼Œåº”ç”¨äºäººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¦‚è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚

(1) ç±»ä¼¼äºNumPyçš„å¼ é‡è®¡ç®—ï¼Œèƒ½åœ¨GPUæˆ–MPSç­‰ç¡¬ä»¶åŠ é€Ÿå™¨ä¸ŠåŠ é€Ÿã€‚

(2) åŸºäºå¸¦è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿçš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚

### 2. Tensor çš„åˆ›å»º
##### 2.1 æŒ‰å†…å®¹åˆ›å»º

```python
# ç›´æ¥åˆ›å»ºæ ‡é‡
tensor1 = torch.tensor(10.0)
print(tensor1)
print(tensor1.size()) 
print(tensor1.dtype) 

# æ ¹æ®liståˆ›å»ºå¼ é‡ -- ä¸€ç»´æ•°ç»„
tensor2 = torch.tensor([i for i in range(10)])
print(tensor2)
print(tensor2.size()) 
print(tensor2.dtype) 

# æ ¹æ®liståˆ›å»ºå¼ é‡ -- äºŒç»´æ•°ç»„
tensor3 = torch.tensor(np.array([[j for j in range(10)] for i in range(5)]))
print(tensor3)
print(tensor3.size()) 
print(tensor3.dtype) 
```

##### 2.2 åˆ›å»ºæŒ‡å®šå½¢çŠ¶çš„å¼ é‡

æ­¤æ—¶ï¼Œè°ƒç”¨çš„æ–¹æ³•æ˜¯torch.Tensor(size),torch.Tensorä¹Ÿå¯ä»¥æŒ‰å†…å®¹åˆ›å»ºå¼ é‡ï¼Œä½†ä¸torch.tensorä¸åŒï¼Œtorch.Tensoré»˜è®¤æ•°æ®ç±»å‹ä¸ºfloat32

```python
tensor = torch.Tensor(3, 2, 4)
print(tensor.size())
print(tensor.shape)
print(tensor.dtype)
```

##### 2.3 æŒ‡å®šç±»å‹åˆ›å»º

å¯ä»¥é€šè¿‡torch.IntTensor()ã€torch.FloatTensor()ã€torch.DoubleTensor()ç­‰æ–¹æ³•æŒ‡å®štensorçš„æ•°æ®ç±»å‹ã€‚æˆ–è€…æ˜¯é€šè¿‡torch.tensor()æ–¹æ³•æŒ‡å®šæ•°æ®ç±»å‹dtypeã€‚

å¯¹äºTensorï¼Œæœ‰long, int, short, byte, double, float, half, bool

å¯¹äºtensorï¼Œæœ‰int64, int32, int16, uint8ï¼ˆæ— ç¬¦å·ï¼‰, float64, float32, float16, bool

```python
# int(32)
tensor1 = torch.tensor([0 for i in range(10)], dtype=torch.int32)
tensor2 = torch.IntTensor(10)
print(tensor1)
print(tensor2)

# long
tensor1 = torch.tensor([i for i in range(5)], dtype=torch.int64)
tensor2 = torch.LongTensor(10)
print(tensor1)
print(tensor2)

# double
tensor1 = torch.tensor([1, 2, 3], dtype=torch.float16)
tensor2 = torch.HalfTensor(3, 4)
print(tensor1)
print(tensor2) 
```

##### 2.4 æŒ‰ç…§åŒºé—´åˆ›å»ºä¸€ç»´å¼ é‡

```python
# torch.arange(start, end, step) å‰é—­åå¼€
tensor1 = torch.arange(10, 30, 2)
tensor2 = torch.arange(6)
 
# torch.linspace(start, end, num) å‰é—­åé—­
tensor3 = torch.linspace(10, 30, 5)

# torch.logspace(start, end, steps, base=10.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
tensor4 = torch.logspace(10, 30, 5, base=2)
print(tensor4)
```

##### 2.5 æŒ‰æ•°å€¼å¡«å……åˆ›å»ºå¼ é‡

```python
# torch.zeros(size)
tensor = torch.zeros(3, 4)
# torch.ones(size)
tensor = torch.ones(3, 4)   
# torch.full(size, value)
tensor = torch.full((3, 4), 5)
# torch.empty(size)
tensor_empyt = torch.empty(3, 4)
# torch.zeros_like(tensor)
tensor_zeros_like = torch.zeros_like(tensor)
# torch.ones_like(tensor)
tensor_ones_like = torch.ones_like(tensor)
# torch.full_like(size, value)
tensor_full_like = torch.full_like(tensor, 666)
# torch.empty_like(tensor)
tensor_empty_like = torch.empty_like(tensor)

# torch.eye(size) ç”Ÿæˆå•ä½çŸ©é˜µ
tensor = torch.eye(3)

		
```

##### 2.6 æŒ‰éšæœºå€¼åˆ›å»ºå¼ é‡

1ï¼‰torch.rand(size)åˆ›å»ºåœ¨ \[0,1)ä¸Šå‡åŒ€åˆ†å¸ƒçš„ï¼ŒæŒ‡å®šå½¢çŠ¶çš„å¼ é‡

2ï¼‰torch.randint(low, high, size)åˆ›å»ºåœ¨ \[low,high)ä¸Šå‡åŒ€åˆ†å¸ƒçš„ï¼ŒæŒ‡å®šå½¢çŠ¶çš„å¼ é‡

3ï¼‰torch.randn(size)åˆ›å»ºæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ï¼ŒæŒ‡å®šå½¢çŠ¶çš„å¼ é‡

4ï¼‰torch.normal(mean,std,size)åˆ›å»ºè‡ªå®šä¹‰æ­£æ€åˆ†å¸ƒçš„ï¼ŒæŒ‡å®šå½¢çŠ¶çš„å¼ é‡

5ï¼‰torch.rand_like(input)åˆ›å»ºåœ¨ \[0,1)ä¸Šå‡åŒ€åˆ†å¸ƒçš„ï¼Œä¸ç»™å®šå¼ é‡å½¢çŠ¶ç›¸åŒçš„å¼ é‡

6ï¼‰torch.randint_like(input, low, high)åˆ›å»ºåœ¨ \[low,high)ä¸Šå‡åŒ€åˆ†å¸ƒçš„ï¼Œä¸ç»™å®šå¼ é‡å½¢çŠ¶ç›¸åŒçš„å¼ é‡

7ï¼‰torch.randn_like(input)åˆ›å»ºæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ï¼Œä¸ç»™å®šå¼ é‡å½¢çŠ¶ç›¸åŒçš„å¼ é‡

```python
# torch.rand(size)
tensor = torch.rand(3, 4) # x ~ U(0,1)

# torch.randn(size)
tensor = torch.randn(3, 4) # x ~ N(0,1)

# torch.randint(low, high, size) å‰é—­åå¼€
tensor = torch.randint(0, 10, (10, 10)) # x ~ U(low,high) 

# torch.normal(mean, std, size) # x ~ N(mean,std)
tensor = torch.normal(*[0, 1], size=(3, 4))

# å…¶ä»–å°±æ˜¯ä¸‰ä¸ªä¸ä¸Šé¢æ–¹æ³•å¯¹åº”çš„likeæ–¹æ³•ï¼Œæ³¨æ„ï¼štorch.normalæ²¡æœ‰likeæ–¹æ³•
```

##### 2.7 éšæœºæ’åˆ—ä¸éšæœºç§å­

``` python
tensor = torch.randperm(10) # å…ˆç”Ÿæˆä¸€ä¸ªåºåˆ—ï¼Œå†éšæœºæ‰“ä¹±
print(tensor)

# æŸ¥çœ‹éšæœºæ•°ç§å­
print(torch.random.initial_seed())
# è®¾ç½®éšæœºæ•°ç§å­
torch.manual_seed(42)
```
### 3. Tensor çš„è½¬æ¢

##### 3.1 å¼ é‡çš„ç±»å‹è½¬æ¢ï¼ˆä¸æ˜¯åŸåœ°æ“ä½œï¼Œè€Œæ˜¯å¼€è¾Ÿæ–°çš„å†…å­˜ç©ºé—´ï¼‰

1ï¼‰Tensor.type(torch.dtype)

```python
tensor = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
print(tensor.dtype)
tensor = tensor.type(torch.float32)
print(tensor.dtype)
```

2ï¼‰Tensor.xxxç±»å‹().. or Tensor.to(xxxç±»å‹)

```python
tensor = torch.Tensor(3, 4) 
# ä¸¤ç§è½¬æ¢æ–¹å¼
tensor1 = tensor.to(torch.float64)
tensor2 = tensor.to(torch.int32)
tensor3 = tensor.short()
# è¾“å‡ºç»“æœ
print(tensor.dtype)
print(tensor1.dtype)
print(tensor2.dtype)
print(tensor3.dtype)
```
##### 3.2 Tensor ä¸ Ndarray çš„è½¬æ¢

1ï¼‰Tensor è½¬ Ndarray $\to$ Tensor.numpyï¼ˆï¼‰æ³¨ï¼šè¯¥æ–¹æ³•ç”Ÿæˆçš„ Ndarray ä¸ åŸå§‹ Tensor å…±äº«åŒä¸€å—åœ°å€ç©ºé—´

```python
# è®¾ç½®æ‰“å°æ–¹å¼---å’Œnumpyå®Œå…¨ä¸€æ ·
np.set_printoptions(precision=6)
torch.set_printoptions(precision=6)
# tensor è½¬ä¸º numpy tensor.numpy()  
tensor = torch.randn(3, 3)
arr = tensor.numpy()
print(tensor) 
arr[0, :] = -1.0
print(arr)
print(tensor)



# é¿å…å…±äº«åŒä¸€å†…å­˜ç©ºé—´
tensor = torch.randn(2, 3)
arr = tensor.numpy().copy()

print(tensor)
arr[0, 0] = 0
print(tensor)
```

2ï¼‰torch.from_numpy(array) å°† numpy æ•°ç»„è½¬æ¢æˆ Tensor

```python
arr = np.array([[j * 1.0 for j in range(5)] for i in range(3)])
# å…±äº«å†…å­˜ï¼Œå¦‚æœæƒ³ä¸å…±äº«å†…å­˜çš„è¯ï¼Œå¯ä»¥ç”¨torch.tensor(array)
tensor = torch.from_numpy(arr)
tensor_t = torch.tensor(arr)
arr[:, 0] = 666.666

print(tensor_t)
print(tensor)
```
##### 3.3 Tensor è½¬ä¸ºæ ‡é‡

å½“ Tensor åªæœ‰ä¸€ä¸ªå…ƒç´ æ—¶ï¼Œå¯ä»¥ç”¨ Tensor.item() å°†å®ƒè½¬ä¸ºæ ‡é‡

```python
# å½“tensoråªæœ‰ä¸€ä¸ªå…ƒç´ æ—¶ï¼ŒTensor.item()å¯ä»¥æå–æ”¹æ•°å€¼
print(torch.randn(1).item())  
```
##### 3.4 Tensor è½¬ä¸ºè½¬ç§©

```python
# 1.å¦‚æœåªå¯¹æœ€åäºŒç»´è½¬ç§©ï¼Œä¹Ÿå°±æ˜¯å¯¹çŸ©é˜µè½¬ç§©ï¼Œç›´æ¥x.mTå³å¯
tensor1.mT
# 2.è‡ªç”±çš„è½¬ç§©ä¸¤ç»´
tensor1.transpose(dim1, dim2)
# 3.é‡æ–°æ’åˆ—æ‰€æœ‰ç»´åº¦
tensor1.permute(dim1, dim1, ... , dimn)
```
### 4. Tensor ä¹‹é—´çš„è¿ç®—

å¼ é‡ä¹‹é—´çš„å¤§éƒ¨åˆ†è¿ç®—éƒ½æœ‰ä¸‰ä¸ªï¼Œå¦‚ +ã€Tensor.add()ã€Tensor.add_()ï¼Œå½“ç„¶ï¼Œæœ‰äº›è¿ç®—æ²¡æœ‰é‡è½½è¿ç®—ç¬¦ï¼Œæ¯”å¦‚ Tensor.log()ã€Tensor.exp()

å…¶ä¸­ã€å¸¦ä¸‹åˆ’çº¿çš„ä»£è¡¨åŸåœ°æ“ä½œï¼Œä¹Ÿå°±æ˜¯èŠ‚çœå†…å­˜ï¼Œå¯¹äºåŸºæœ¬è¿ç®—ï¼Œç­‰ä»·äº x += y ï¼ˆä¸å¼€è¾Ÿæ–°çš„å†…å­˜ï¼Œç›´æ¥æŠŠç»“æœæ”¾åˆ° x ä¸Šï¼‰

#### 4.1 åŸºæœ¬è¿ç®—

##### 1. å››åˆ™è¿ç®—

+, -, *, / åŠ å‡ä¹˜é™¤ï¼Œä¸æ”¹å˜åŸæ•°æ®

t.add(), t.sub(), t.mul(), t.div() åŠ å‡ä¹˜é™¤ï¼Œä¸æ”¹å˜åŸæ•°æ®

t.add_(), t.sub_(), t.mul_(), t.div_() åŠ å‡ä¹˜é™¤ï¼Œæ”¹å˜åŸæ•°æ®

##### 2. -ã€neg()ã€neg_()

```python
tensor1 = torch.tensor([1, 2, 3])
print(-tensor1)
print()

print(tensor1.neg())
print(tensor1)
print()

print(tensor1.neg_())
print(tensor1)
```

##### 3. **ã€pow()ã€pow_()

```python
tensor = torch.tensor([1, 2, 3], dtype=torch.float32)
print(tensor**2)

print(tensor.pow(2))
print(tensor)

print(tensor.pow_(2))
print(tensor)
```

##### 4. sqrt()ã€sqrt_()

```python
print(tensor.sqrt())
print(tensor.sqrt_())
```

##### 5. å…¶ä»–è¿ç®—

log(), log_()

exp(), exp_()

#### 4.2 **å“ˆè¾¾ç›ç§¯ t1 \* t2 | t1.mul(t2)**

```python
# å¼ é‡çš„å¯¹åº”ä½ç½®å…ƒç´ ç›¸ä¹˜ï¼Œè¿›è¡Œè®¡ç®—çš„å¼ é‡ç»´åº¦ç›¸åŒ
tensor1 = torch.tensor([[1, 3], [2, 4]])
tensor2 = torch.tensor([[2, 4], [1, 3]])
print(tensor1.mul(tensor2))
```

#### 4.3 å¼ é‡ä¹˜æ³•

mm()ä¸¥æ ¼ç”¨äºäºŒç»´çŸ©é˜µç›¸ä¹˜ã€‚

@ã€matmul()æ”¯æŒå¤šç»´å¼ é‡ï¼ŒæŒ‰æœ€åä¸¤ä¸ªç»´åº¦åšçŸ©é˜µä¹˜æ³•ï¼Œå…¶ä»–ç»´åº¦ç›¸åŒï¼Œæˆ–è€…è‡³å°‘ä¸€ä¸ªå¼ é‡å¯¹åº”ç»´åº¦ä¸º1ï¼Œå¹¿æ’­åè¿›è¡Œè¿ç®—ã€‚

```python
tensor1 = torch.randn(2, 2, 3, 3)
tensor2 = torch.randn(2, 2, 3, 10)
t = tensor1 @ tensor2
print(t.size())
```

### 5. Tensor çš„ä¸€äº›ç»Ÿè®¡æ–¹æ³•

1) sum()æ±‚å’Œ

2) mean()æ±‚å‡å€¼

3) max()/min()æ±‚æœ€å¤§/æœ€å°å€¼åŠå…¶ç´¢å¼•

4) argmax()/argmin()æ±‚æœ€å¤§å€¼/æœ€å°å€¼çš„ç´¢å¼•

5) std()æ±‚æ ‡å‡†å·®

6) unique()å»é‡

7) sort()æ’åº

æ³¨ï¼šè¿™å†™æ–¹æ³•ææ¸…æ¥šç»´åº¦ dim çš„å½±å“å³å¯ï¼Œéå¸¸ç®€å•ã€‚
### 6. Tensor çš„ç´¢å¼•æ“ä½œ

##### 6.1 ç®€å•ç´¢å¼•

ä¹Ÿå°±æ˜¯ç›´æ¥ç»™ä¸ªåæ ‡å°±å®Œäº‹äº†ã€‚

```python
# 1. ç®€å•ç´¢å¼•  ç›´æ¥ç»™åæ ‡
print(tensor1[1, 2, 3])
```

##### 6.2 èŒƒå›´ç´¢å¼•

ä»¥ä¸‰ç»´å¼ é‡ä¸¾ä¾‹,å…¶ä»–ç»´åº¦ä¸€æ · t[st:ed:step, st:ed:step, st:ed:step]ï¼Œ tensorè¦æ±‚æ­¥é•¿å¿…é¡»å¤§äº0, è€Œnumpyåˆ™æ²¡æœ‰é™åˆ¶

```python
print(tensor1[:, :, 0:2])
print(tensor1[0, ::2, :]) 
```

#####  6.3 åˆ—è¡¨ç´¢å¼•

1ï¼‰å„ç»´åº¦ç´¢å¼•åˆ—è¡¨é•¿åº¦å¿…é¡»ä¸€è‡´ï¼Œå®ç°ä¸€å¯¹ä¸€å…ƒç´ é€‰æ‹©

```python
print(tensor1[[0, 1, 2], [1, 2, 3], [1, 2, 3]])
print(tensor1[[0, 1, 2], [0, 1, 2]])
```

2ï¼‰**åˆ—è¡¨åµŒå¥—ç´¢å¼•**ï¼šæ”¯æŒå¹¿æ’­æœºåˆ¶ï¼Œå†…å±‚åˆ—è¡¨å¯å®ç°ä¸€å¯¹å¤šæ˜ å°„å…³ç³»

```python
# åˆ—è¡¨çš„åµŒå¥— : ä¸åµŒå¥—æ˜¯ä¸€å¯¹ä¸€, åµŒå¥—å°±æ˜¯ ä¸€å¯¹å¤š
print(tensor1[[[0], [1]], [1, 2]])
```

##### 6.4 å¸ƒå°”ç´¢å¼•

å¸ƒå°”ç´¢å¼• : ç”¨æ¡ä»¶åˆ¤æ–­è¡¨è¾¾å¼,ç”Ÿæˆä¸€ä¸ªé«˜ç»´å¸ƒå°”æ•°ç»„,å†æŠŠè¿™ä¸ªé«˜ç»´å¸ƒå°”æ•°ç»„ä½œä¸ºç´¢å¼•,è¿”å›æ»¡è¶³æ¡ä»¶çš„å…ƒç´ 

1ï¼‰é€‰æ‹©æŸäº›è¡Œ

```python
# é€‰å–ç¬¦åˆæ¡ä»¶çš„æŸäº›è¡Œ:æ¯”å¦‚è¯´é¦–å…ƒç´ å¤§äº5
mask = tensor1[:, :, 0] > 5			
print(tensor1[mask])
```

2ï¼‰é€‰æ‹©æŸäº›åˆ—

```python
# é€‰æ‹©æ‰€æœ‰åˆ—çš„é¦–å…ƒç´ å°äº3çš„åˆ—
mask = tensor1[:,0,:] < 3 # å½“å‰maskæ˜¯(3, 4), è€Œtensor1æ˜¯(3, 5, 4),æ‰€ä»¥ä¸èƒ½ç›´æ¥åº”ç”¨

# å…ˆæŠŠ tensor1 è½¬ç§©ä¸º (3, 4, 5), ç„¶åå†åº”ç”¨
tensor2 = tensor1.mT
print(tensor2[mask].mT)
```

3ï¼‰é€‰æ‹©æŸäº›çŸ©é˜µ

```python
# é€‰å–ç¬¦åˆæ¡ä»¶çš„çŸ©é˜µ
# æ¯”å¦‚å½“å‰è¦é€‰æ‹©çš„çŸ©é˜µæ˜¯(1, 2) > 5
mask = tensor1[:, 1, 2] > 5
print(mask)
print(tensor1[mask])
```

4ï¼‰é€‰æ‹©æŸäº›å…ƒç´ 

```python
mask = tensor1 > 5
print(tensor1[mask])
```

### 7. Tensor çš„å½¢çŠ¶æ“ä½œ

##### 7.1 ç»´åº¦äº¤æ¢

1ï¼‰Tensor.transpose()äº¤æ¢ä¸¤ä¸ªç»´åº¦  æ³¨æ„ï¼šåœ¨numpyä¸­ï¼Œtransposeå°±æ˜¯permute

```python
print(tensor.transpose(0,1).shape)
```

2ï¼‰Tensor.permute

```python
tensor = torch.rand(3, 4, 5)
print(tensor.permute(2, 1, 0).shape)
```

3ï¼‰Tensor.mTï¼šç›´æ¥äº¤æ¢çŸ©é˜µçš„ç»´åº¦

##### 7.2 Tensor çš„å½¢çŠ¶è½¬æ¢ï¼ˆarrayçš„Reshape)

1ï¼‰Tensor.reshape(size) `reshape` å¹¶ä¸æ€»æ˜¯å¼€è¾Ÿé¢å¤–å†…å­˜ï¼Œåªæœ‰åœ¨å¿…è¦æ—¶æ‰ä¼šåˆ›å»ºæ–°çš„å†…å­˜å‰¯æœ¬ã€‚

```python
# reshape
tensor = torch.randint(1, 10, (3, 5, 4))
print(tensor.reshape(3, -1))
```

2ï¼‰Tensor.view(size) éœ€è¦å†…å­˜è¿ç»­ã€‚å…±äº«å†…å­˜

```python
# view
tensor.is_contiguous()
tensor.view(-1, 3)
```



##### 7.3 å¢åŠ æˆ–åˆ é™¤ç»´åº¦

1ï¼‰Tensor.unsqueeze(dim) å¢åŠ ç»´åº¦

```python
# åŸå¼ é‡å½¢çŠ¶: torch.Size([3, 4])
tensor = torch.rand(3, 4)
t1 = tensor.unsqueeze(0)  # å½¢çŠ¶å˜ä¸º: torch.Size([1, 3, 4])
print(t1.shape)
t2 = tensor.unsqueeze(1)  # å½¢çŠ¶å˜ä¸º: torch.Size([3, 1, 4])
print(t2.shape)
t3 = tensor.unsqueeze(-1) # å½¢çŠ¶å˜ä¸º: torch.Size([3, 4, 1])
print(t3.shape)
```

2ï¼‰Tensor.squeeze(dim=None) åˆ é™¤å¼ é‡ä¸­å¤§å°ä¸º1çš„ç»´åº¦

è¯´æ˜ï¼šå½“æŒ‡å®š `dim` å‚æ•°æ—¶ï¼Œåªåˆ é™¤æŒ‡å®šç»´åº¦ï¼ˆå¦‚æœè¯¥ç»´åº¦å¤§å°ä¸º1ï¼‰å½“ä¸æŒ‡å®š `dim` å‚æ•°æ—¶ï¼Œåˆ é™¤æ‰€æœ‰å¤§å°ä¸º1çš„ç»´åº¦

```python
# åŸå¼ é‡å½¢çŠ¶: torch.Size([1, 3, 1, 4])
tensor = torch.rand(1, 3, 1, 4)

t1 = tensor.squeeze()      # åˆ é™¤æ‰€æœ‰å¤§å°ä¸º1çš„ç»´åº¦ï¼Œå½¢çŠ¶å˜ä¸º: torch.Size([3, 4])
print(t1.shape)
t2 = tensor.squeeze(0)     # åˆ é™¤ç¬¬0ç»´ï¼Œå½¢çŠ¶å˜ä¸º: torch.Size([3, 1, 4])
print(t2.shape)
t3 = tensor.squeeze(2)     # åˆ é™¤ç¬¬2ç»´ï¼Œå½¢çŠ¶å˜ä¸º: torch.Size([1, 3, 4])
print(t3.shape)
t4 = tensor.squeeze(-2)    # åˆ é™¤å€’æ•°ç¬¬2ç»´ï¼Œå½¢çŠ¶å˜ä¸º: torch.Size([1, 3, 4])
print(t4.shape)
```



### 8. Tensor çš„æ‹¼æ¥æ“ä½œ

##### 8.1 torch.cat(tensors, dim=0, *, out=None)

- æ²¿æŒ‡å®šç»´åº¦è¿æ¥å¤šä¸ªå¼ é‡
- é™¤è¿æ¥ç»´åº¦å¤–ï¼Œå…¶ä»–ç»´åº¦çš„å¤§å°å¿…é¡»ç›¸åŒ

```python
 # torch.cat
tensor1 = torch.randint(1, 10, (2, 3, 5))
tensor2 = torch.randint(1, 10, (2, 2, 5))
t = torch.cat([tensor1, tensor2], dim=1)
t
```


##### 8.2 torch.stack(tensors, dim=0, *, out=None) 

### 9. Pytorch çš„è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼štorch.autograd

è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œæ¡†æ¶ä¼šæ ¹æ®è®¾è®¡å¥½çš„æ¨¡å‹æ„å»ºä¸€ä¸ªè®¡ç®—å›¾ï¼ˆcomputational graphï¼‰ï¼Œæ¥è·Ÿè¸ªè®¡ç®—æ˜¯å“ªäº›æ•°æ®é€šè¿‡å“ªäº›æ“ä½œç»„åˆèµ·æ¥äº§ç”Ÿè¾“å‡ºï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­ç®—æ³•æ¥æ ¹æ®ç»™å®šå‚æ•°çš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦è°ƒæ•´å‚æ•°ï¼ˆæ¨¡å‹æƒé‡ï¼‰ã€‚

PyTorchå…·æœ‰ä¸€ä¸ªå†…ç½®çš„å¾®åˆ†å¼•æ“torch.autogradä»¥æ”¯æŒè®¡ç®—å›¾çš„æ¢¯åº¦è‡ªåŠ¨è®¡ç®—ã€‚

è€ƒè™‘æœ€ç®€å•çš„å•å±‚ç¥ç»ç½‘ç»œï¼Œå…·æœ‰è¾“å…¥xã€å‚æ•°wã€åç½®bä»¥åŠæŸå¤±å‡½æ•°ï¼š

<p align='center'>
    <img src="../../assets/imgs/python/pytorch/pytorch07.png" alt="pytorch07" style="zoom:80%;" />
</p>

```python
import torch

# å®šä¹‰è¾“å…¥æ•°æ®å’ŒçœŸå®æ ‡ç­¾
x = torch.tensor([[1.0]])  # è¾“å…¥æ•°æ®
y_true = torch.tensor([[2.0]])  # çœŸå®æ ‡ç­¾

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼Œrequires_grad=Trueè¡¨ç¤ºéœ€è¦è®¡ç®—æ¢¯åº¦
w = torch.randn(1, 1, requires_grad=True)  # æƒé‡
b = torch.randn(1, requires_grad=True)  # åç½®

# å‰å‘ä¼ æ’­ï¼šè®¡ç®— z = x * w + b
z = x * w + b
print(z)  # è¾“å‡ºå¸¦æœ‰grad_fnçš„è®¡ç®—ç»“æœ

# å®šä¹‰æŸå¤±å‡½æ•°å¹¶è®¡ç®—æŸå¤±å€¼
loss = torch.nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
loss_value = loss(z, y_true)  # è®¡ç®—é¢„æµ‹å€¼zä¸çœŸå®å€¼y_trueçš„æŸå¤±
print(loss_value)  # è¾“å‡ºæŸå¤±å€¼

# åå‘ä¼ æ’­ï¼šè‡ªåŠ¨è®¡ç®—æ¢¯åº¦
loss_value.backward()

# è¾“å‡ºå‚æ•°çš„æ¢¯åº¦
print(w.grad)  # æƒé‡çš„æ¢¯åº¦
print(b.grad)  # åç½®çš„æ¢¯åº¦

# æ£€æŸ¥å¼ é‡æ˜¯å¦ä¸ºå¶å­èŠ‚ç‚¹
print(x.is_leaf)  # True: ç”¨æˆ·åˆ›å»ºçš„å¼ é‡
print(y_true.is_leaf)  # True: ç”¨æˆ·åˆ›å»ºçš„å¼ é‡
print(z.is_leaf)  # False: è®¡ç®—å¾—åˆ°çš„ä¸­é—´å˜é‡
print(loss_value.is_leaf)  # False: è®¡ç®—å¾—åˆ°çš„ä¸­é—´å˜é‡
```

è‡ªåŠ¨å¾®åˆ†çš„å…³é”®å°±æ˜¯è®°å½•èŠ‚ç‚¹çš„æ•°æ®ä¸è¿ç®—ã€‚æ•°æ®è®°å½•åœ¨å¼ é‡çš„dataå±æ€§ä¸­ï¼Œè®¡ç®—è®°å½•åœ¨å¼ é‡çš„grad_fnå±æ€§ä¸­ã€‚

è®¡ç®—å›¾æ ¹æ®æ­å»ºæ–¹å¼å¯åˆ†ä¸ºé™æ€å›¾å’ŒåŠ¨æ€å›¾ï¼ŒPyTorchæ˜¯åŠ¨æ€å›¾æœºåˆ¶ï¼Œåœ¨è®¡ç®—çš„è¿‡ç¨‹ä¸­é€æ­¥æ­å»ºè®¡ç®—å›¾ï¼ŒåŒæ—¶å¯¹æ¯ä¸ªTensoréƒ½å­˜å‚¨grad_fnä¾›è‡ªåŠ¨å¾®åˆ†ä½¿ç”¨ã€‚

**è‹¥è®¾ç½®å¼ é‡å‚æ•°requires_grad=Trueï¼Œåˆ™PyTorchä¼šè¿½è¸ªæ‰€æœ‰åŸºäºè¯¥å¼ é‡çš„æ“ä½œ**ï¼Œå¹¶åœ¨åå‘ä¼ æ’­æ—¶è®¡ç®—å…¶æ¢¯åº¦ã€‚ä¾èµ–äºå¶å­èŠ‚ç‚¹çš„èŠ‚ç‚¹ï¼Œrequires_gradé»˜è®¤ä¸ºTrueã€‚å½“è®¡ç®—åˆ°æ ¹èŠ‚ç‚¹åï¼Œåœ¨æ ¹èŠ‚ç‚¹è°ƒç”¨backward()æ–¹æ³•å³å¯åå‘ä¼ æ’­è®¡ç®—è®¡ç®—å›¾ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„æ¢¯åº¦ã€‚

**éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦åœ¨åå‘ä¼ æ’­ä¹‹åä¼šè¢«é‡Šæ”¾æ‰ï¼ˆé™¤éè®¾ç½®å‚æ•°retain_grad=Trueï¼‰**ã€‚è€Œå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦åœ¨åå‘ä¼ æ’­ä¹‹åä¼šä¿ç•™ï¼ˆç´¯ç§¯ï¼‰ã€‚é€šå¸¸éœ€è¦ä½¿ç”¨**optimizer.zero_grad()**æ¸…é›¶å‚æ•°çš„æ¢¯åº¦ã€‚

### 10. Tensor ä¸è®¡ç®—å›¾çš„åˆ†ç¦»

æœ‰æ—¶æˆ‘ä»¬å¸Œæœ›å°†æŸäº›è®¡ç®—ç§»åŠ¨åˆ°è®¡ç®—å›¾ä¹‹å¤–ï¼Œå¯ä»¥ä½¿ç”¨**Tensor.detach()**è¿”å›ä¸€ä¸ªæ–°çš„å˜é‡ï¼Œè¯¥**å˜é‡ä¸åŸå˜é‡å…·æœ‰ç›¸åŒçš„å€¼**ï¼ˆå…±äº«å†…å­˜ï¼‰ï¼Œä½†ä¸¢å¤±è®¡ç®—å›¾ä¸­å¦‚ä½•è®¡ç®—åŸå˜é‡çš„ä¿¡æ¯ã€‚æ¢å¥è¯è¯´ï¼Œæ¢¯åº¦ä¸ä¼šåœ¨è¯¥å˜é‡å¤„ç»§ç»­å‘ä¸‹ä¼ æ’­ã€‚

```python
# å½“æˆ‘ä»¬éœ€è¦å¯¹è®¡ç®—å›¾ä¸­çš„æŸäº›å¼ é‡è¿›è¡Œç‹¬ç«‹å¤„ç†ï¼ŒåŒæ—¶åˆä¸å¸Œæœ›å½±å“åŸæœ‰è®¡ç®—å›¾ç»“æ„æ—¶ï¼Œå¯ä»¥é‡‡ç”¨ä»¥ä¸‹å‡ ç§è§£å†³æ–¹æ¡ˆï¼š
x = torch.rand(2, 2, requires_grad=True)
y = x.detach()
print(x)
print(y)
# å°½ç®¡ x ä¸ y çš„ id ä¸åŒï¼Œä½†ä»–ä»¬çš„æ•°æ®æ˜¯å…±äº«çš„ã€‚ 
print(x.untyped_storage().data_ptr())
print(y.untyped_storage().data_ptr())   
```

 	å¦‚æœåœ¨ä¸€ä¸ªä»£ç æ®µä¸­éƒ½ä¸å¸Œæœ›å¯¹è®¡ç®—å›¾äº§ç”Ÿå½±å“ï¼Œç›´æ¥ç”¨ï¼šwith torch.no_grad() ä¸Šä¸‹æ–‡ç®¡ç†å™¨å³å¯ã€‚

å½“ç„¶ï¼Œæœ€ç®€å•ç²—æš´çš„æ–¹å¼å°±æ˜¯tensor.clone()ï¼Œç„¶åå¯¹å‰¯æœ¬è¿›è¡Œæ“ä½œã€‚

## Pytorch è¿›é˜¶

### 1 æ¿€æ´»å‡½æ•°

```python
# åœ¨pytorchä¸­ï¼Œèƒ½ç”¨åˆ°çš„æ¿€æ´»å‡½æ•°å‡ ä¹éƒ½è¢«å®ç°äº†ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨ä»–ä»¬ã€‚

torch.sigmoid(input)
torch.tanh(input)
torch.relu(input, inplace=False)
torch.relu6(input, inplace=False)
torch.leaky_relu(input, negative_slope=0.01, inplace=False)
torch.prelu(input, weight)
torch.elu(input, alpha=1.0, inplace=False)
torch.selu(input, inplace=False)
torch.gelu(input, approximate='none')
torch.swish(input)
torch.mish(input)
torch.logsigmoid(input)
torch.softplus(input, beta=1, threshold=20)
torch.softmax(input, dim)
torch.log_softmax(input, dim)
torch.hardsigmoid(input, inplace=False)
torch.hardswish(input, inplace=False)
torch.hardtanh(input, min_val=-1.0, max_val=1.0, inplace=False)
```

### 2 ä¸€äº›ç¥ç»ç½‘ç»œå±‚

##### 1. nn.Linear ç±»

```python
torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)

# é‡è¦å±æ€§
linear.weight
linear.bias
```

##### x. nn.Dropout ç±»

```python
# è¿™æ˜¯æ‰€æœ‰Dropoutçš„çˆ¶ç±»
class _DropoutNd(Module):
    __constants__ = ["p", "inplace"]
    p: float
    inplace: bool

    def __init__(self, p: float = 0.5, inplace: bool = False) -> None:
        super().__init__()
        if p < 0 or p > 1:
            raise ValueError(
                f"dropout probability has to be between 0 and 1, but got {p}"
            )
        self.p = p
        self.inplace = inplace

    def extra_repr(self) -> str:
        return f"p={self.p}, inplace={self.inplace}"

# å®ä¾‹åŒ–dropoutå®ä¾‹
dropout = nn.Dropout(p, inplace)

# è°ƒç”¨forward
dropout(input)
```

### 3 å‚æ•°çš„åˆå§‹åŒ–

å‚æ•°çš„åˆå§‹åŒ–ï¼šå¯ä»¥æŠŠå¯¹åº”å‚æ•°çš„dataæ‹¿å‡ºæ¥ç›´æ¥æ”¹ï¼Œæˆ–è€…ç”¨nn.init()å»ä¿®æ”¹ã€‚å…·ä½“åˆå§‹åŒ–æ–¹æ³•å¦‚ä¸‹ï¼š

##### 1. å¸¸æ•°åˆå§‹åŒ–

```python
# å¸¸æ•°åˆå§‹åŒ–
# æ–¹æ³•ä¸€ï¼Œç›´æ¥å…¨éƒ¨ç½®ä¸º0 nn.init.zeros_(input)
nn.init.zeros_(linear1.weight)
print(linear1.weight)
# æ–¹æ³•äºŒï¼Œå¯ä»¥ç½®ä¸ºä»»æ„å¸¸æ•° nn.init.constant_(input, value)
nn.init.constant_(linear1.weight,0)
print(linear1.weight)
```

##### 2. ç§©åˆå§‹åŒ–

```python
nn.init.eye_(linear.weight)
```

##### 3. éšæœºåˆå§‹åŒ–

```python
# æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
nn.init.normal_(linear1.weight, mean=0, std=0.01)
print(linear1.weight)

# å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
nn.init.uniform_(linear1.weight, -1, 1)
```

##### 4. Xavieråˆå§‹ï¼ˆå½“åé¢çš„æ¿€æ´»å‡½æ•°æ˜¯ï¼šSigmoid, Tanh è¿™ç±» S å‹æ›²çº¿ï¼‰

```python
# Xavier åˆå§‹åŒ–
nn.init.xavier_normal_(linear1.weight)
print(linear1.weight)
nn.init.xavier_uniform_(linear1.weight)
print(linear1.weight)
```

##### 5. He åˆå§‹åŒ–ï¼ˆKaiMing åˆå§‹åŒ–ï¼Œä¸»è¦ç”¨åˆ°åé¢æ˜¯Relu	è¿™ç±»æ¿€æ´»å‡½æ•°çš„Linearï¼‰

```python
# KaiMing åˆå§‹åŒ–
nn.init.kaiming_normal_(linear1.weight)
print(linear1.weight)

nn.init.kaiming_uniform_(linear1.weight)
print(linear1.weight)
```

### 4 æ­å»ºç¥ç»ç½‘ç»œ

##### 1. è‡ªå®šä¹‰æ¨¡å‹

åœ¨ç¥ç»ç½‘ç»œæ¡†æ¶ä¸­ï¼Œç”±å¤šä¸ªå±‚ç»„æˆçš„ç»„ä»¶ç§°ä¹‹ä¸º **æ¨¡å—ï¼ˆModuleï¼‰**ã€‚

åœ¨PyTorchä¸­æ¨¡å‹å°±æ˜¯ä¸€ä¸ª**Moduleï¼Œå„ç½‘ç»œå±‚ã€æ¨¡å—ä¹Ÿæ˜¯Module**ã€‚Moduleæ˜¯æ‰€æœ‰ç¥ç»ç½‘ç»œçš„åŸºç±»ã€‚

åœ¨å®šä¹‰ä¸€ä¸ªModuleæ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç»§æ‰¿torch.nn.Moduleå¹¶ä¸»è¦å®ç°ä¸¤ä¸ªæ–¹æ³•ï¼š

 	\__init__ï¼šå®šä¹‰ç½‘ç»œå„å±‚çš„ç»“æ„ï¼Œå¹¶åˆå§‹åŒ–å‚æ•°ã€‚

forwardï¼šæ ¹æ®è¾“å…¥è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¹¶è¿”å›è¾“å‡ºã€‚è®¡ç®—å…¶è¾“å‡ºå…³äºè¾“å…¥çš„æ¢¯åº¦ï¼Œå¯é€šè¿‡å…¶åå‘ä¼ æ’­å‡½æ•°è¿›è¡Œè®¿é—®ï¼ˆé€šå¸¸è‡ªåŠ¨å‘ç”Ÿï¼‰ã€‚forwardæ–¹æ³•æ˜¯æ¯æ¬¡è°ƒç”¨çš„å…·ä½“å®ç°ã€‚

```python
class MyModel(nn.Module):
    
    def __init__(self):
        super().__init__()
        # å®šä¹‰ç½‘ç»œç»“æ„
        self.linear1 = nn.Linear(3, 4)
        self.linear2 = nn.Linear(4, 4)
        self.linear3 = nn.Linear(4, 2)

        # åˆå§‹åŒ–ç½‘ç»œå‚æ•°
        nn.init.xavier_normal_(self.linear1.weight)
        nn.init.kaiming_normal_(self.linear2.weight)
    

    def forward(self, x):
        z1 = torch.tanh(self.linear1(x))
        z2 = torch.relu(self.linear2(z1))
        return torch.softmax(self.linear3(z2), dim=1)
```

##### 2. æŸ¥çœ‹æ¨¡å‹å‚æ•°

```python
# æŸ¥çœ‹æ¨¡å‹å‚æ•°
# æœ€ç®€å•çš„æ–¹æ³• ç›´æ¥ä»å¯¹åº”çš„å±‚ä¸­æŸ¥çœ‹
print(net.linear1.weight)

# ç¬¬äºŒç§ï¼Œè°ƒç”¨model.parameters()è·å–ä¸åŒå±‚çš„å‚æ•°ï¼Œä¸€èˆ¬ç”¨net.named_parameters()ï¼Œå¯ä»¥è·å¾—å½“å‰å±‚çš„åå­—
for name, param in net.named_parameters():
    print(name, '\n', param.data)

# ç¬¬ä¸‰ç§ï¼Œå»å¾—åˆ°ä¿å­˜å‚æ•°çš„dict
d = net.state_dict()
for k, v in d.items():
    print(k, v)
```

##### 3. æŸ¥çœ‹æ¨¡å‹ç»“æ„å’Œå‚æ•°æ•°é‡

ç¬¬ä¸‰æ–¹åº“ï¼štorchsummary.summary(net, input_size, batch_size, device)

```python
# æŸ¥çœ‹æ¨¡å‹ç»“æ„ä¸å‚æ•°æ•°é‡
from torchsummary import summary
summary(net, (3,), batch_size=10, device='cpu')
```



##### 4. device çš„è®¾ç½®

ä¸ç”¨åˆ›å»ºä¸€å±‚æŒ‡å®šä¸€å±‚ï¼Œè€Œæ˜¯åœ¨åˆ›å»ºæ¨¡å‹æ—¶ç›´æ¥ç»™ä¸€ä¸ª device å€¼å°±å¯ä»¥äº†ï¼Œæˆ–è€…åˆ›å»ºå®Œä»¥åç”¨ net.to("cuda")

```python
# åœ¨åˆ›å»ºå¼ é‡çš„æ—¶å€™æŒ‡å®šè®¾å¤‡
input = torch.randn(1, 3, 224, 224, device='cuda')
print(input.device)

# é€šè¿‡ tensor.to å»å°†å¼ é‡æ¢åˆ° cuda ä¸Š
tensor.to(device='cuda')
```



##### 5. ä½¿ç”¨ Sequential æ„å»ºæ¨¡å‹ éå¸¸å¥½ä½¿ï¼ï¼ï¼ï¼

å¯ä»¥é€šè¿‡**torch.nn.Sequential**æ¥æ„å»ºæ¨¡å‹ï¼Œå°†**å„å±‚æŒ‰é¡ºåº**ä¼ å…¥ã€‚**æ³¨ï¼šæ­¤æ—¶æŠŠæ¿€æ´»å‡½æ•°ä¹Ÿå½“æˆäº†ä¸€å±‚ï¼Œæ‰€ä»¥è¦è°ƒnn.Tanhï¼Œè€Œä¸å†æ˜¯ä¼ ç»Ÿå‰å‘ä¼ æ’­ç›´æ¥ torch.xxx å‡½æ•°è¿™æ ·å»è®¡ç®—äº†ã€‚**

Sequentialç±»ä½¿æ¨¡å‹æ„é€ å˜å¾—ç®€å•ï¼Œä¸å¿…è‡ªå®šä¹‰ç±»å°±å¯ä»¥ç»„åˆæ–°çš„æ¶æ„ã€‚ç„¶è€Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„æ¶æ„éƒ½æ˜¯ç®€å•çš„é¡ºåºæ¶æ„ï¼Œå½“éœ€è¦æ›´å¼ºçš„çµæ´»æ€§æ—¶è¿˜æ˜¯éœ€è¦è‡ªå®šä¹‰æ¨¡å‹ã€‚

```python
# ä½¿ç”¨sequentialå»åˆ›å»ºæ¨¡å‹
model = nn.Sequential(
    nn.Linear(3, 4),
    nn.Tanh(),
    nn.Linear(4, 4),
    nn.ReLU(),
    nn.Linear(4, 2),
    nn.Softmax(dim=1)
)
print(model)
```

`model.apply()` æ˜¯PyTorchä¸­ `nn.Module` ç±»çš„ä¸€ä¸ªé‡è¦æ–¹æ³•ï¼Œç”¨äºå¯¹æ¨¡å‹ä¸­çš„**æ¯ä¸ªæ¨¡å—é€’å½’åœ°åº”ç”¨ä¸€ä¸ªå‡½æ•°**ã€‚

```  
# å‚æ•°åˆå§‹åŒ–
def init_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)
        
model.apply(init_weights) 
```

### 5 æŸå¤±å‡½æ•°

##### 1. BCE

äºŒåˆ†ç±»ä»»åŠ¡å¸¸ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆBinary Cross-Entropy Lossï¼‰

æ³¨ï¼šè¿™é‡Œçš„ input å’Œ target ç»´åº¦ä¸€å®šè¦ä¸€æ ·ï¼Œå¦‚æœæ˜¯ï¼ˆ10ï¼Œ3ï¼‰ï¼Œé‚£ä¹ˆå°±ä»£è¡¨ç€æœ‰10ä¸ªæ ·æœ¬ï¼Œè¦å¯¹æ¯ä¸ªæ ·æœ¬åšä¸‰ä¸ªå¹¶è¡Œçš„äºŒåˆ†ç±»ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªæ ·æœ¬çš„ targer å¦‚æœæ˜¯ï¼ˆ1ï¼Œ1ï¼Œ0ï¼‰ï¼Œå°±ä»£è¡¨è¿™ä¸ªæ ·æœ¬æ˜¯ç±»åˆ«0ï¼Œç±»åˆ«1ï¼Œä¸æ˜¯ç±»åˆ«2ã€‚


$$
L = -\frac{1}{n} \sum_{i=1}^{n} \left( y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i) \right)
$$

```python
# pytorchä»£ç ï¼š
loss = nn.BCELoss()
loss_val = loss(y_hat, t)
```

##### 2. å¤šåˆ†ç±»ä»»åŠ¡æŸå¤±å‡½æ•°

å¤šåˆ†ç±»ä»»åŠ¡å¸¸ç”¨å¤šç±»äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆCategorical Cross-Entropy Lossï¼‰ã€‚å®ƒæ˜¯å¯¹æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ ‡ç­¾ä¹‹é—´å·®å¼‚çš„åŠ æƒå¹³å‡ã€‚

$$
\text{CE}(y, \hat{y}) = - \sum_{i=1}^{C} y_i \log(\hat{y}_i) \\
\mathcal{L}_{\text{CE}} = - \log(\hat{y}_c)
$$

åœ¨PyTorchä¸­å¯ä½¿ç”¨torch.nn.CrossEntropyLoss å®ç°ï¼š(è‡ªåŠ¨åšäº†Softmaxï¼Œå…¶æ¬¡ï¼Œtargetå¯ä»¥æ˜¯ç±»åˆ«ç´¢å¼•ï¼Œç±»å‹å¿…é¡»ä¸ºlongï¼Œè¿˜å¯ä»¥æ˜¯ç‹¬çƒ­ç¼–ç ï¼Œä¹Ÿå°±æ˜¯ç›¸å¯¹äºçš„æ¦‚ç‡åˆ†å¸ƒ)

```python
import torch
import torch.nn as nn

# çœŸå®å€¼ä¸ºæ ‡ç­¾
target = torch.tensor([1, 0, 3, 2, 5, 4]) Â # çœŸå®å€¼
input = torch.randn((6, 8)) Â # é¢„æµ‹å€¼
loss = nn.CrossEntropyLoss() Â # å®ä¾‹åŒ–æŸå¤±å‡½æ•°
print(loss(input, target))

# çœŸå®å€¼ä¸ºæ¦‚ç‡
target = torch.randn(6, 8).softmax(dim=1) Â # çœŸå®å€¼
input = torch.randn((6, 8)) Â # é¢„æµ‹å€¼
loss = nn.CrossEntropyLoss() Â # å®ä¾‹åŒ–æŸå¤±å‡½æ•°
print(loss(input, target))
```



##### 3. MAE

å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Errorï¼ŒMAEï¼‰ï¼Œä¹Ÿç§°L1 Lossï¼ŒL2 å°±æ˜¯å–äº†ä¸ªå¹³æ–¹é¡¹ï¼Œæ•…çœç•¥ä¸å†™ã€‚

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

```python
l1 = nn.L1Loss()
l2 = nn.MSELoss()
```

 

### 6 ä¼˜åŒ–ç®—æ³•

##### 1. Momentum

è€Œ Momentumï¼ˆåŠ¨é‡æ³•ï¼‰ä¼šä¿å­˜å†å²æ¢¯åº¦å¹¶ç»™äºˆä¸€å®šçš„æƒé‡ï¼Œä½¿å…¶ä¹Ÿå‚ä¸åˆ°å‚æ•°æ›´æ–°ä¸­ï¼š

$$
v \leftarrow \alpha v - \eta \nabla\\
W \leftarrow W + v
$$

åŠ¨é‡æ³•æ¼”ç¤ºå¦‚ä¸‹ï¼šé€šè¿‡åŠ¨é‡æ³•å»æ‰¾åˆ°å‡½æ•° f(x) çš„æœ€å°å€¼ï¼Œæ•ˆæœå¦‚ä¸‹å›¾ï¼š

<p align='center'>
    <img src="../../assets/imgs/python/pytorch/pytorch01.png" alt="pytorch01" style="zoom:80%;" />
</p>

##### 2. å­¦ä¹ ç‡è¡°å‡

**1ï¼‰ç­‰é—´éš”è¡°å‡** 

```python
torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)
# ç”¨æ³•
# å®šä¹‰å­¦ä¹ ç‡è¡°å‡
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
# æ›´æ–°å­¦ä¹ ç‡
scheduler.step()
```

| å‚æ•°å          | ç±»å‹                      | å«ä¹‰                                           |
| ------------ | ----------------------- | -------------------------------------------- |
| `optimizer`  | `torch.optim.Optimizer` | è¦è°ƒæ•´å­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨å¯¹è±¡ï¼ˆä¾‹å¦‚ `optim.SGD`ã€`optim.Adam` ç­‰ï¼‰ã€‚ |
| `step_size`  | `int`                   | æ¯éš”å¤šå°‘ä¸ª **epoch** ä¸‹é™ä¸€æ¬¡å­¦ä¹ ç‡ã€‚                     |
| `gamma`      | `float`, é»˜è®¤ `0.1`       | å­¦ä¹ ç‡è¡°å‡å› å­ï¼ˆä¹˜æ³•å› å­ï¼‰ã€‚å³ï¼š`new_lr = old_lr * gamma`ã€‚   |
| `last_epoch` | `int`, é»˜è®¤ `-1`          | ä¸Šä¸€ä¸ª epoch çš„ç¼–å·ï¼ˆç”¨äºä»ä¸­æ–­å¤„æ¢å¤è®­ç»ƒæ—¶æŒ‡å®šï¼‰ã€‚                |
| `verbose`    | `bool`, é»˜è®¤ `False`      | è‹¥ä¸º `True`ï¼Œæ¯æ¬¡æ›´æ–°å­¦ä¹ ç‡æ—¶ä¼šæ‰“å°æ—¥å¿—ä¿¡æ¯ã€‚                   |

<p align='center'>
    <img src="../../assets/imgs/python/pytorch/pytorch02.png" alt="pytorch02" style="zoom:80%;" />
</p>

2ï¼‰æŒ‡å®šé—´éš”è¡°å‡

```python
torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma)
# milestones æ˜¯ä¸€ä¸ªå¯è¿­ä»£å®¹å™¨ï¼Œå­˜æ”¾è¦åœ¨é‚£äº› epoch è¡°å‡å­¦ä¹ ç‡
```

<p align='center'>
    <img src="../../assets/imgs/python/pytorch/pytorch03.png" alt="pytorch03" style="zoom:80%;" />
</p>



3ï¼‰æŒ‡æ•°è¡°å‡

```python
scheduler_expo = optim.lr_scheduler.ExponentialLR(optimizer, 0.99)
```

<p align='center'>
    <img src="../../assets/imgs/python/pytorch/pytorch04.png" alt="pytorch04" style="zoom:80%;" />
</p>

##### 3. Adagrad

```python
optimizer = optim.Adagrad([X], lr=lr)
```

##### 4. RMSprop

ç›¸è¾ƒäº Adagradï¼ŒRMSProp åœ¨åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡çš„æ–¹å¼ä¸Šè¿›è¡Œäº†æ”¹è¿›ã€‚åœ¨ Adagrad ä¸­ï¼Œå­¦ä¹ ç‡ä¼šè¢«è¿‡å»æ‰€æœ‰æ¢¯åº¦å¹³æ–¹çš„ç´¯ç§¯å’Œæ‰€é™¤ï¼Œä»¥æ­¤å®ç°è‡ªé€‚åº”è°ƒæ•´ã€‚æ¢¯åº¦å¹³æ–¹å’Œè¶Šå¤§ï¼Œè¯´æ˜è¯¥å‚æ•°å·²ç»è¢«å……åˆ†å­¦ä¹ ï¼Œå› è€Œå…¶å­¦ä¹ ç‡ä¼šé€æ¸å‡å°ã€‚ç„¶è€Œï¼Œè¿™ç§ç´¯ç§¯æ–¹å¼ä¼šå¯¼è‡´åœ¨è®­ç»ƒåæœŸå­¦ä¹ ç‡è¶‹è¿‘äºé›¶ï¼Œä»è€Œå½±å“æ¨¡å‹çš„è¿›ä¸€æ­¥æ”¶æ•›ã€‚

ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒRMSProp å¼•å…¥äº†**æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼ˆExponential Moving Average, EMAï¼‰\**æœºåˆ¶ï¼Œç”¨äºè®¡ç®—æ¢¯åº¦å¹³æ–¹çš„åŠ æƒå¹³å‡å€¼ï¼Œä»è€Œ\**é€æ­¥é—å¿˜æ—§æ¢¯åº¦ä¿¡æ¯**ï¼Œä¿æŒå­¦ä¹ ç‡åœ¨ä¸€ä¸ªåˆé€‚çš„åŠ¨æ€èŒƒå›´å†…ã€‚ å…¶æ ¸å¿ƒæ€æƒ³å¯è¡¨ç¤ºä¸ºï¼š

$$
h \leftarrow ah + (1-a)\nabla^2
\\
W \leftarrow W - \eta \frac{1}{\sqrt{h}} \nabla
$$

å…¶ä¸­ï¼Œ$\alpha$ è¡¨ç¤ºè¡°å‡ç³»æ•°ï¼Œç”¨äºæ§åˆ¶å†å²æ¢¯åº¦çš„â€œé—å¿˜é€Ÿåº¦â€ã€‚

```python
optimizer = optim.RMSprop([X], lr=lr, alpha=0.2)
```


##### 5. Adam
```
pass
```

## CNN

### 1 CNNæ¦‚è¿°

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Networkï¼ŒCNNï¼‰å¸¸è¢«ç”¨äºå›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ç­‰å„ç§åœºåˆã€‚å®ƒåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ã€‚

å·ç§¯ç¥ç»ç½‘ç»œçš„çµæ„Ÿæ¥è‡ªäºåŠ¨ç‰©è§†è§‰çš®å±‚ç»„ç»‡çš„ç¥ç»è¿æ¥æ–¹å¼ï¼Œå•ä¸ªç¥ç»å…ƒåªå¯¹æœ‰é™åŒºåŸŸå†…çš„åˆºæ¿€ä½œå‡ºååº”ï¼Œä¸åŒç¥ç»å…ƒçš„æ„ŸçŸ¥åŒºåŸŸç›¸äº’é‡å ä»è€Œè¦†ç›–æ•´ä¸ªè§†é‡ã€‚

CNNä¸­æ–°å‡ºç°äº†å·ç§¯å±‚ï¼ˆConvolutionå±‚ï¼‰å’Œæ± åŒ–å±‚ï¼ˆPoolingå±‚ï¼‰ï¼Œä¸‹å›¾æ˜¯ä¸€ä¸ªCNNçš„ç»“æ„ï¼š

<p align='center'>
    <img src="../../assets/imgs/python/pytorch/pytorch05.png" alt="pytorch05" style="zoom:80%;" />
</p>

### 2 è¾“å‡ºç»´åº¦è®¡ç®—å…¬å¼

å‡è®¾è¾“å…¥æ•°æ®å½¢çŠ¶ä¸º $(H, W)$ï¼Œå·ç§¯æ ¸å¤§å°ä¸º $(FH, FW)$ï¼Œå¡«å……ï¼ˆpaddingï¼‰ä¸º $P$ï¼Œæ­¥å¹…ï¼ˆstrideï¼‰ä¸º $S$ï¼Œè¾“å‡ºæ•°æ®å½¢çŠ¶ä¸º $(OH, OW)$ï¼Œåˆ™å¯å¾—ï¼š

$$
OH = \frac{H + 2P - FH}{S} + 1
$$

$$
OW = \frac{W + 2P - FW}{S} + 1
$$

---

#### ğŸ“Œ ç¤ºä¾‹

ä¾‹å¦‚ï¼Œå¯¹äºå½¢çŠ¶ä¸º $(4, 4)$ çš„è¾“å…¥æ•°æ®ï¼Œåº”ç”¨å¹…åº¦ä¸º 1 çš„å¡«å……ï¼ˆå³ $P = 1$ï¼‰ï¼Œå¹¶ä½¿ç”¨æ­¥å¹… $S = 3$ï¼Œå·ç§¯æ ¸å¤§å°ä¸º $(3, 3)$ çš„å·ç§¯è¿ç®—ï¼š

$$
OH = OW = \frac{4 + 2 \times 1 - 3}{3} + 1 = \frac{3}{3} + 1 = 2
$$

å¾—åˆ°å½¢çŠ¶ä¸º $(2, 2)$ çš„è¾“å‡ºæ•°æ®ã€‚

---

#### âš ï¸ æ³¨æ„äº‹é¡¹

å½“è¾“å‡ºå¤§å°æ— æ³•æ•´é™¤æ—¶ï¼ŒPyTorch å·ç§¯å±‚ä¼šè‡ªåŠ¨å‘ä¸‹å–æ•´ï¼Œè¾“å‡ºæ•´æ•°å°ºå¯¸ï¼Œèˆå¼ƒæ— æ³•è¦†ç›–å®Œæ•´å·ç§¯æ ¸çš„è¾“å…¥éƒ¨åˆ†ã€‚

âœ… ä¹Ÿå°±æ˜¯è¯´ï¼šå®é™…è®¡ç®—ä¸­ä½¿ç”¨çš„æ˜¯ **åœ°æ¿é™¤æ³•**ï¼ˆfloor divisionï¼‰ï¼Œå³ï¼š

$$
OH = \left\lfloor \frac{H + 2P - FH}{S} \right\rfloor + 1
$$

---

#### ğŸ’¡ å°è´´å£«

| å‚æ•°       | å«ä¹‰            |
| -------- | ------------- |
| $H, W$   | è¾“å…¥é«˜åº¦ã€å®½åº¦       |
| $FH, FW$ | å·ç§¯æ ¸é«˜åº¦ã€å®½åº¦      |
| $P$      | å¡«å……å¤§å°ï¼ˆpaddingï¼‰ |
| $S$      | æ­¥å¹…ï¼ˆstrideï¼‰    |
| $OH, OW$ | è¾“å‡ºé«˜åº¦ã€å®½åº¦       |

### 3 Pytorch ä¸­çš„ CNN

#### 1ã€å·ç§¯å±‚

åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ï¼ŒäºŒç»´å·ç§¯å±‚ï¼ˆå¦‚ PyTorch çš„ `Conv2d`ï¼‰çš„åˆå§‹åŒ–å‡½æ•°é€šå¸¸å®šä¹‰äº†å·ç§¯çš„åŸºæœ¬å‚æ•°ã€‚ä»¥ä¸‹æ˜¯å…¶æ ‡å‡†å‡½æ•°ç­¾ååŠå‚æ•°è¯´æ˜ã€‚

**âœ… PyTorch é£æ ¼å‡½æ•°ç­¾å**

```python
torch.nn.Conv2d(
    in_channels,      # è¾“å…¥é€šé“æ•°ï¼ˆC_inï¼‰
    out_channels,     # è¾“å‡ºé€šé“æ•°ï¼ˆC_outï¼‰
    kernel_size,      # å·ç§¯æ ¸å¤§å°ï¼ˆFH, FWï¼‰
    stride=1,         # æ­¥å¹…ï¼ˆSï¼‰
    padding=0,        # å¡«å……å¤§å°ï¼ˆPï¼‰
    dilation=1,       # è†¨èƒ€ç‡ï¼ˆç©ºæ´å·ç§¯ï¼‰
    groups=1,         # åˆ†ç»„å·ç§¯å‚æ•°
    bias=True,        # æ˜¯å¦ä½¿ç”¨åç½®é¡¹
    padding_mode='zeros'
)
```

#### 2ã€æ± åŒ–å±‚

æ± åŒ–å±‚ç”¨äºå¯¹ç‰¹å¾å›¾è¿›è¡Œä¸‹é‡‡æ ·ï¼ˆdownsamplingï¼‰ï¼Œé™ä½ç©ºé—´ç»´åº¦ï¼ˆé«˜åº¦å’Œå®½åº¦ï¼‰ï¼Œå‡å°‘è®¡ç®—é‡å¹¶å¢å¼ºç‰¹å¾çš„å°ºåº¦ä¸å˜æ€§ã€‚å¸¸è§ç±»å‹æœ‰æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰å’Œå¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰ã€‚

```python
torch.nn.MaxPool2d(
    kernel_size,      # æ± åŒ–çª—å£å¤§å° (H_k, W_k)
    stride=None,      # æ­¥å¹…ï¼Œè‹¥ä¸º None åˆ™é»˜è®¤ç­‰äº kernel_size
    padding=0,        # å¡«å……å¤§å°
    dilation=1,       # è†¨èƒ€ç‡ï¼ˆé€šå¸¸ç”¨äºç©ºæ´æ± åŒ–ï¼Œè¾ƒå°‘ä½¿ç”¨ï¼‰
    return_indices=False,  # æ˜¯å¦è¿”å›æœ€å¤§å€¼ç´¢å¼•ï¼ˆç”¨äº MaxUnpoolï¼‰
    ceil_mode=False   # æ˜¯å¦ä½¿ç”¨å‘ä¸Šå–æ•´æ¨¡å¼
)	
```

$$
H_{out} = \left\lfloor \frac{H_{in} + 2p - d \times (k - 1) - 1}{s} + 1 \right\rfloor
$$

$$
W_{out} = \left\lfloor \frac{W_{in} + 2p - d \times (k - 1) - 1}{s} + 1 \right\rfloor
$$



å½“ d = 1æ—¶ï¼Œä¹Ÿå°±æ˜¯å¤§éƒ¨åˆ†æƒ…å†µï¼š

$$
H_{out} = \left\lfloor \frac{H_{in} + 2p - k}{s} \right\rfloor + 1
$$


## NLP å¤„ç†

### 1. è¯åµŒå…¥å±‚

#### 1.1 ä»€ä¹ˆæ˜¯è¯åµŒå…¥

ç„¶è¯­è¨€æ˜¯ç”±æ–‡å­—æ„æˆçš„ï¼Œè€Œè¯­è¨€çš„å«ä¹‰æ˜¯ç”±å•è¯æ„æˆçš„ã€‚å³å•è¯æ˜¯å«ä¹‰çš„æœ€å°å•ä½ã€‚å› æ­¤ä¸ºäº†è®©è®¡ç®—æœºç†è§£è‡ªç„¶è¯­è¨€ï¼Œé¦–å…ˆè¦è®©å®ƒç†è§£å•è¯å«ä¹‰ã€‚

è¯å‘é‡æ˜¯ç”¨äºè¡¨ç¤ºå•è¯æ„ä¹‰çš„å‘é‡ï¼Œä¹Ÿå¯ä»¥çœ‹ä½œè¯çš„ç‰¹å¾å‘é‡ã€‚å°†è¯æ˜ å°„åˆ°å‘é‡çš„æŠ€æœ¯ç§°ä¸º **è¯åµŒå…¥**ï¼ˆWord Embeddingï¼‰ã€‚  

è¿˜æœ‰ä¸€ç§ä½¿ç”¨å‘é‡è¡¨ç¤ºå•è¯æ„ä¹‰çš„æ–¹å¼æ˜¯ç‹¬çƒ­å‘é‡ï¼Œç‹¬çƒ­å‘é‡å¾ˆå®¹æ˜“æ„å»ºï¼Œä½†å®ƒä»¬é€šå¸¸ä¸æ˜¯ä¸€ä¸ªå¥½çš„é€‰æ‹©ã€‚ä¸€ä¸ªä¸»è¦åŸå› æ˜¯ç‹¬çƒ­å‘é‡ä¸èƒ½å‡†ç¡®è¡¨è¾¾ä¸åŒè¯ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚æ¯”å¦‚ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ $\frac{x^{T}y}{||x||||y||}$ æ¥è¡¨ç¤ºä¸¤ä¸ªè¯ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œç”±äºä»»æ„ä¸¤ä¸ªä¸åŒè¯çš„ç‹¬çƒ­å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ä¸º0ï¼Œæ‰€ä»¥ç‹¬çƒ­å‘é‡ä¸èƒ½ç¼–ç è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚å¦ä¸€ä¸ªåŸå› æ˜¯éšç€è¯æ±‡é‡çš„å¢å¤§ï¼Œç‹¬çƒ­å‘é‡è¡¨ç¤ºçš„å‘é‡å¤§å°ä¹Ÿä¼šå¢å¤§ï¼Œåœ¨è¯æ±‡é‡è¾ƒå¤§çš„æƒ…å†µä¸‹ä¼šæ¶ˆè€—å¤§é‡çš„å­˜å‚¨èµ„æºä¸è®¡ç®—èµ„æºã€‚

å°†è¯è½¬æ¢ä¸ºè¯å‘é‡æ—¶ï¼š

1ï¼‰é¦–å…ˆéœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå†æ ¹æ®éœ€è¦è¿›è¡Œæ¸…æ´—å’Œæ ‡å‡†åŒ–ã€‚

2ï¼‰æ„å»ºè¯è¡¨ï¼ˆVocabularyï¼‰ï¼Œæ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ªç´¢å¼•ã€‚

3ï¼‰ä½¿ç”¨è¯åµŒå…¥çŸ©é˜µå°†è¯ç´¢å¼•è½¬æ¢ä¸ºè¯å‘é‡ã€‚

#### 1.2 API ä½¿ç”¨

å¯ä½¿ç”¨torch.nn.Embeddingæ¥åˆå§‹åŒ–è¯åµŒå…¥çŸ©é˜µï¼š

```python
torch.nn.Embedding(num_embeddings, embedding_dim)
# num_embeddings:è¯çš„æ•°é‡
# embedding_dim:è¯å‘é‡çš„ç»´åº¦
```

```python
text = "è‡ªç„¶è¯­è¨€æ˜¯ç”±æ–‡å­—æ„æˆçš„ï¼Œè€Œè¯­è¨€çš„å«ä¹‰æ˜¯ç”±å•è¯æ„æˆçš„ã€‚å³å•è¯æ˜¯å«ä¹‰çš„æœ€å°å•ä½ã€‚å› æ­¤ä¸ºäº†è®©è®¡ç®—æœºç†è§£è‡ªç„¶è¯­è¨€ï¼Œé¦–å…ˆè¦è®©å®ƒç†è§£å•è¯å«ä¹‰ã€‚"
original_words = jieba.lcut(text)
print(original_words)

# è‡ªå®šä¹‰ä¸€ç»„åœç”¨è¯
stopwords = {'çš„','ï¼Œ' ,'ã€‚', 'æ˜¯','è€Œ','ç”±'}

# è¿‡æ»¤ä¸éœ€è¦çš„è¯å’Œæ ‡ç‚¹ç¬¦åˆ
words = []
for word in original_words:
    if word not in stopwords: words.append(word)
print(words)

# æ„å»ºè¯è¡¨
id2word = list(set(words))
print(id2word)

# æ„å»ºå­—å…¸ï¼Œè®°å½•wordtoid
word2id = {}
for i, word in enumerate(id2word):
    word2id[word] = i
print(word2id)

# æ„å»ºä¸€ä¸ªåµŒå…¥å±‚
embed = nn.Embedding(len(id2word), 5)

# å‰å‘ä¼ æ’­ï¼Œä¼ å…¥ç´¢å¼•å·ï¼Œå¾—åˆ°è¯å‘é‡
for k,v in word2id.items():
    word_emd = embed(torch.tensor(v))
    print(f"{v:>2}:{k:8}\t{word_emd.detach().numpy()}")
```


### 2. RNN

##### 2.1 RNN ä»‹ç»

æ–‡æœ¬æ˜¯è¿ç»­çš„ï¼Œå…·æœ‰åºåˆ—ç‰¹æ€§ã€‚å¦‚æœå…¶åºåˆ—è¢«é‡æ’å¯èƒ½å°±ä¼šå¤±å»åŸæœ‰çš„æ„ä¹‰ã€‚æ¯”å¦‚â€œç‹—å’¬äººâ€è¿™æ®µæ–‡æœ¬å…·æœ‰åºåˆ—å…³ç³»ï¼Œå¦‚æœæ–‡å­—çš„åºåˆ—é¢ å€’å¯èƒ½å°±ä¼šè¡¨è¾¾ä¸åŒçš„æ„æ€ã€‚

ç›®å‰æˆ‘ä»¬æ¥è§¦çš„ç¥ç»ç½‘ç»œéƒ½æ˜¯å‰é¦ˆå‹ç¥ç»ç½‘ç»œã€‚å‰é¦ˆï¼ˆfeedforwardï¼‰æ˜¯æŒ‡ç½‘ç»œçš„ä¼ æ’­æ–¹å‘æ˜¯å•å‘çš„ã€‚å…·ä½“åœ°è¯´ï¼Œå°†è¾“å…¥ä¿¡å·ä¼ ç»™ä¸‹ä¸€å±‚ï¼Œä¸‹ä¸€å±‚æ¥æ”¶åˆ°ä¿¡å·åä¼ ç»™ä¸‹ä¸‹ä¸€å±‚ï¼Œç„¶åå†ä¼ ç»™ä¸‹ä¸‹ä¸‹ä¸€å±‚â€¦åƒè¿™æ ·ï¼Œä¿¡å·ä»…åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šä¼ æ’­ã€‚è™½ç„¶å‰é¦ˆç½‘ç»œç»“æ„ç®€å•ã€æ˜“äºç†è§£ï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºè®¸å¤šä»»åŠ¡ä¸­ã€‚ä¸è¿‡ï¼Œè¿™ç§ç½‘ç»œå­˜åœ¨ä¸€ä¸ªå¤§é—®é¢˜ï¼Œå°±æ˜¯ä¸èƒ½å¾ˆå¥½åœ°å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ã€‚æ›´ç¡®åˆ‡åœ°è¯´ï¼Œå•çº¯çš„å‰é¦ˆç½‘ç»œæ— æ³•å……åˆ†å­¦ä¹ æ—¶åºæ•°æ®çš„æ€§è´¨ã€‚äºæ˜¯ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networkï¼ŒRNNï¼‰åº”è¿è€Œç”Ÿã€‚

RNNå±‚å…·æœ‰ç¯è·¯ï¼Œé€šè¿‡ç¯è·¯æ•°æ®å¯ä»¥åœ¨å±‚å†…å¾ªç¯ã€‚å‘æ—¶åºæ•°æ®è¾“å…¥å±‚ä¸­$x0,x1...xt$ï¼Œç›¸åº”çš„ä¼šè¾“å‡º$h0,h1...ht$

<p align='center'>
    <img src="../../assets/imgs/python/pytorch/pytorch06.png" alt="pytorch06" style="zoom:80%;" />
</p>

ç”±å›¾å¯çŸ¥ï¼Œå„ä¸ªæ—¶åˆ»çš„RNNå±‚æ¥æ”¶ä¼ ç»™è¯¥å±‚çš„è¾“å…¥ $x_t$ å’Œå‰ä¸€ä¸ªæ—¶åˆ»RNNå±‚çš„è¾“å‡º $x_{t-1}$ æ®æ­¤è®¡ç®—å½“å‰æ—¶åˆ»RNNå±‚çš„è¾“å‡º $h_t$ ï¼š


$$
h_t = tanh(h_{t-1}W_h + x_tW_x + b)
$$


##### 2.2 API ä»‹ç»

å¯ä½¿ç”¨torch.nn.RNNæ¥åˆå§‹åŒ–RNNå±‚ï¼š

åˆå§‹åŒ– rnn å±‚

```python
rnn = torch.nn.RNN(input_size, hidden_size, num_layers)
# input_size:è¾“å…¥æ•°æ®çš„ç‰¹å¾æ•°é‡
# hidden_size:éšè—çŠ¶æ€çš„ç‰¹å¾æ•°é‡
# num_layers:éšè—å±‚çš„å±‚æ•°ï¼Œå¦‚æœè®¾ç½®å¤šä¸ªå±‚ï¼Œå‰ä¸€ä¸ªéšè—å±‚çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªéšè—å±‚çš„è¾“å…¥
```

è°ƒç”¨æ—¶éœ€è¦ä¼ å…¥2ä¸ªå‚æ•°ï¼š

```python
è°ƒç”¨æ—¶éœ€è¦ä¼ å…¥2ä¸ªå‚æ•°ï¼š
output, hn = rnn(input, hx)
# input:è¾“å…¥æ•°æ®[seq_lenåºåˆ—é•¿åº¦, batch_sizeæ‰¹é‡å¤§å°, input_size]
# hx:åˆå§‹éšçŠ¶æ€[num_layers, batch_size, hidden_size]
# output:è¾“å‡ºæ•°æ®[seq_len, batch_size, hidden_size]
# hn:éšçŠ¶æ€[num_layers, batch_size, hidden_size]
```

 
